{"_id": "3ac3eef636db11635a21a61804cb28e92c546a5686dd1e12", "task": "qasper", "score": 0.0, "response": "unanswerable", "gold": ["Ground truth is not established in the paper"], "input_len": 4101}
{"_id": "db6c4d611b78c840972b57ee0b245003566b982feaff1084", "task": "qasper", "score": 0.48, "response": "An extension of NetVLAD approach that adds ghost clusters for aggregating local descriptors into global features.", "gold": ["extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters", "An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content."], "input_len": 3438}
{"_id": "e419f2bff9d2ab7c3b60b3250caccd2d9ae1285ec3e8e818", "task": "qasper", "score": 0.125, "response": "68.8% to 71.8%", "gold": ["the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"], "input_len": 4488}
{"_id": "9fb085a1f47673d1907f2378c90843b4b6e8622a14fe1fa9", "task": "qasper", "score": 0.3333333333333333, "response": "Context tweets are proposed as additional features.", "gold": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "input_len": 3089}
{"_id": "14e88916523197e066dbb743339685621db47c4beb503abb", "task": "qasper", "score": 1.0, "response": "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney.", "gold": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "input_len": 4543}
{"_id": "c109a25d23a8c5c95af8697ef1ac51cd3bee93c67521af6d", "task": "qasper", "score": 0.0, "response": "No", "gold": ["Yes", "Yes"], "input_len": 5588}
{"_id": "072d3de1a7122730a13a31db3eede4113af2d920814f0aaa", "task": "qasper", "score": 0.0, "response": "unanswerable", "gold": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "input_len": 5846}
{"_id": "8fa5af6a36dd0b6b73900b2ec6f6e43a652a3e7d2b827a58", "task": "qasper", "score": 0.35294117647058826, "response": "CNN/DailyMail, NYT, XSum", "gold": ["CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum", "the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22"], "input_len": 6686}
{"_id": "5f00d4f6e62f4b99484eb78491f803f8143cc1b13ad33816", "task": "qasper", "score": 0.28571428571428575, "response": "Better correlation on SCWS and WS datasets, and better precision and F1-score on entailment datasets.", "gold": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "input_len": 3581}
{"_id": "91dd7b7a6ead4025763812d70dc51c6674b0acf31bd5a5f0", "task": "qasper", "score": 0.0, "response": "It starts with the best performing model and adds the next best model that improves validation performance, discarding those that do not.", "gold": ["simply averaging the predictions from the constituent single models"], "input_len": 5440}
{"_id": "fe6e5087f91071369646d9d868d784a290d0d13013292b61", "task": "qasper", "score": 0.5217391304347826, "response": "Friends comes from the scripts of the Friends TV sitcom, and EmotionPush is made up of Facebook messenger chats.", "gold": ["Friends TV sitcom, Facebook messenger chats"], "input_len": 4682}
{"_id": "51b9066a5f2845e2fdf0d1dcde6833f70ae49ed01aa306db", "task": "qasper", "score": 1.0, "response": "English", "gold": ["English", "Simple English"], "input_len": 3356}
{"_id": "bae15e10e0f414a92fb0e943871ed25c3fc16183a3028012", "task": "qasper", "score": 0.6666666666666666, "response": "IMDb dataset", "gold": ["IMDb dataset of movie reviews", "IMDb"], "input_len": 3514}
{"_id": "d022dfe02fb2a55b4baa40fe436f616aecd3f3ced1a58d7c", "task": "qasper", "score": 0.0, "response": "unanswerable", "gold": ["F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ", "F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)"], "input_len": 8808}
{"_id": "43279ddf85ada1b163aa1b316a4df1418957058206501c26", "task": "qasper", "score": 0.0, "response": "Yes", "gold": ["No"], "input_len": 4881}
{"_id": "f27a64d129091a6c8973c001ff789b8f68955b8ff0ae70af", "task": "qasper", "score": 0.0, "response": "unanswerable", "gold": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "input_len": 17958}
{"_id": "c47ca982b6c4681c4741d4708801fa79a3e1cab17d0a2c4a", "task": "qasper", "score": 0.0, "response": "unanswerable", "gold": ["Energy with accuracy of 0.538", "Energy"], "input_len": 15909}
{"_id": "1614ea21f8d039debfc655015f8133d0ef220837804ddf62", "task": "qasper", "score": 0.8571428571428571, "response": "RNN-based NMT, Transformer-NMT", "gold": ["RNN-based NMT model, Transformer-NMT"], "input_len": 5284}
{"_id": "159a474e0a7155a2d0b984cbb5215cdc853d721605b90154", "task": "qasper", "score": 0.9767441860465117, "response": "A regularization term associated with neutral features, the maximum entropy of class distribution regularization term, and the KL divergence between reference and predicted class distribution.", "gold": ["a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution", "a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution"], "input_len": 5168}
{"_id": "d426a2d42f3dffc8771498ba64ed0e383b91939398e83dce", "task": "qasper", "score": 0.7733333333333332, "response": "SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embedding, CNN, Recurrent Convolutional Neural Networks (RCNN), SVM and deep learning models with comment information, UTCNN without user information, UTCNN without topic information, UTCNN without comment information.", "gold": ["SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information", "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information"], "input_len": 6065}
{"_id": "680ef767fe2a990615d0bc1cb1bf5f7d23c8766209138826", "task": "qasper", "score": 0.08333333333333333, "response": "They improved the state-of-the-art performance by using a neural network architecture with multitask learning, achieving better performance on fine-grained sentiment classification.具体的数值未在文中给出。", "gold": ["They decrease MAE in 0.34"], "input_len": 3757}
{"_id": "8b6bf313950a892cbda035f2c7b3d8b01472ff34749f028d", "task": "qasper", "score": 0.2941176470588235, "response": "Their model improves interpretability by allowing more confident and sparse attention, making identifying specific roles and specializations of attention heads.", "gold": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "input_len": 7993}
{"_id": "04af9dc96013a1bc7faecbc589f7ea5c207e92c7d9a3495e", "task": "qasper", "score": 0.4, "response": "context--agnostic MT system", "gold": [" MT system on the data released by BIBREF11", "Transformer base, two-pass CADec model"], "input_len": 5295}
{"_id": "3b069a546d5417be546f57d02981cf5ac68b4d3d6c55624a", "task": "qasper", "score": 0.8333333333333333, "response": "XNLI test accuracy, Labeled Attachment Scores (LAS)", "gold": ["translation probabilities, Labeled Attachment Scores (LAS)", "accuracy, Labeled Attachment Scores (LAS)"], "input_len": 5470}
{"_id": "ebd4ae480fe1596841b2132e96f40eac8437c800db8ef59e", "task": "qasper", "score": 0.0, "response": "unanswerable", "gold": ["the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."], "input_len": 7663}
{"_id": "036bfbdbfff8294f59afd5860663bbb4349924c9853b2151", "task": "qasper", "score": 1.0, "response": "unanswerable", "gold": ["Unanswerable"], "input_len": 5037}
{"_id": "8595332098adaabcfd8ae199f754a9b06cdde08cdd4cc64a", "task": "qasper", "score": 0.5, "response": "one LSTM layer", "gold": ["LSTM", "LSTM"], "input_len": 3217}
{"_id": "3ca65b23b3cb316653709b469b4a9b42b790e0350c76ae1a", "task": "qasper", "score": 1.0, "response": "Yes", "gold": ["Unanswerable", "Yes"], "input_len": 9374}
{"_id": "39352d304bd452de69fcc492ad458b72614e329b4509eb48", "task": "qasper", "score": 1.0, "response": "unanswerable", "gold": ["Unanswerable", "LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC"], "input_len": 2924}
{"_id": "18576b9ee9994a46dc0c7d916a009ff6d0964991541010ee", "task": "qasper", "score": 0.6666666666666666, "response": "22,880", "gold": ["22,880 users", "20,000"], "input_len": 5469}
{"_id": "86c75d2a9157cb601a49f8424fa1e6e90fcd724132490bbd", "task": "qasper", "score": 0.3404255319148936, "response": "BPE perplexity, BLEU-1/4, ROUGE-L, Distinct-1/2, recipe-level coherence score, Mean Reciprocal Rank (MRR)", "gold": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "input_len": 4119}
{"_id": "b72f9154e71c03d0403e06e50063325961ea2ad27c245763", "task": "qasper", "score": 0.11764705882352941, "response": "No Answer", "gold": ["(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer", "the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms"], "input_len": 4798}
{"_id": "2ecd2808617740f222a4e2f3b2df546da046d876b1580952", "task": "qasper", "score": 0.0, "response": "unanswerableable", "gold": ["57,505 sentences", "57,505 sentences"], "input_len": 5933}
{"_id": "11be2f14f540e957e9797cc962203b8186ca10561228f81f", "task": "qasper", "score": 0.5, "response": "four machine translation tasks", "gold": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "input_len": 7988}
{"_id": "bfc5d4d72997fdcc0107cd5cab9b9718777c43b86be45eb2", "task": "qasper", "score": 0.0, "response": "unanswerable", "gold": ["5 percent points.", "0.05 F1"], "input_len": 4791}
{"_id": "65f7bdb541fd6f01fe866dcf694891f92533186085fcba20", "task": "qasper", "score": 1.0, "response": "unanswerable", "gold": ["Unanswerable"], "input_len": 10737}
{"_id": "091b4028a3b5e9d8248c58f17a62fd16c878da69693cfbfb", "task": "qasper", "score": 0.0, "response": "unanswerable", "gold": ["No", "No"], "input_len": 3798}
{"_id": "93004cdb0e6d24f5de0568ed952fd50655e42900dc9dbfdb", "task": "qasper", "score": 0.5000000000000001, "response": "Nguni languages and Sotho languages", "gold": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "input_len": 2780}
{"_id": "3598040772b4950e6241b50125f7ab7038a8149060e3c381", "task": "qasper", "score": 0.0, "response": "unanswerable", "gold": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "input_len": 5722}
{"_id": "ecf37304ab84334d46491b0bd3d8bdc7cda33d54a15870f0", "task": "qasper", "score": 0.3636363636363636, "response": "29,794 articles", "gold": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "input_len": 5876}
{"_id": "1a1daeb5809cde7c7131d972f42ee43654a378c78d5182c8", "task": "qasper", "score": 0.0, "response": "unanswerable", "gold": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "input_len": 7159}
{"_id": "d1d93cefe6e2c643ecf128643f9362e3fa137ff54253a3ec", "task": "qasper", "score": 0.0, "response": "unanswerable", "gold": ["Yes", "Yes"], "input_len": 5928}
{"_id": "92da01e7242f30f5266e431b4269fee1b0ca5fcc23aee095", "task": "qasper", "score": 0.18518518518518517, "response": "Efficiency and accuracy are evaluated through retention rate and matching accuracy, respectively.", "gold": ["by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews", "efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence"], "input_len": 2756}
{"_id": "b5f62b7acf975c5454df077adf727c1bcc62985be5e5a660", "task": "qasper", "score": 0.888888888888889, "response": "Accuracy, precision, recall, and F-measure", "gold": ["Precision, Recall, F-measure, accuracy", "Precision, Recall and F-measure"], "input_len": 4367}
{"_id": "c92d96ed55bc5dcd92f963d3c5d26e52661b74e71090f24a", "task": "qasper", "score": 0.0, "response": "Unanswerable", "gold": ["Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen", "we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)"], "input_len": 6945}
{"_id": "eeab2b9167f294f68d3058752acc01e1145eb7d89437d5c4", "task": "qasper", "score": 0.0, "response": "unanswerable", "gold": ["Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM"], "input_len": 4787}
{"_id": "d5280384a4496ef3358dc45dc0199b9198bd0d927be302ef", "task": "qasper", "score": 1.0, "response": "Embedding Layer, Neural Network Layers, Loss Function, Metrics", "gold": ["Embedding Layer, Neural Network Layers, Loss Function, Metrics", "Embedding Layer, Neural Network Layers, Loss Function, Metrics"], "input_len": 2527}
{"_id": "0b90a0a4b2cdceda62a9e2b165f0cfa1a34d34d800a46086", "task": "qasper", "score": 0.7142857142857142, "response": "They used the multilingual pronunciation corpus collected by deri2016grapheme and the Carnegie Mellon Pronouncing Dictionary.", "gold": ["the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary", "multilingual pronunciation corpus collected by deri2016grapheme"], "input_len": 4446}
{"_id": "c9d48df129798d2aed0161fa805a289e5f75306af3e22336", "task": "qasper", "score": 0.0, "response": "unanswerable", "gold": ["varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)"], "input_len": 3457}
{"_id": "443d051a54d96c296d9135dad3794d09f89ff91d6433d092", "task": "qasper", "score": 1.0, "response": "English, Spanish, Finnish", "gold": ["English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish", "English, Spanish, Finnish"], "input_len": 5686}
